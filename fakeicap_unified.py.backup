#!/usr/bin/env python3
"""
FakeICAP Unified Application
Combines ICAP server and web interface in a single application
"""

# Standard library imports
import os
import sys
import socket
import time
import re
import uuid
import threading
import sqlite3
import hashlib
import secrets
import glob
from datetime import datetime, timedelta
from functools import wraps
from pathlib import Path

# Flask imports
from flask import Flask, render_template, request, redirect, url_for, session, flash, jsonify, send_file
from flask_bcrypt import Bcrypt
from werkzeug.utils import secure_filename
import logging.handlers

# Local imports
from logger_config import get_logger, init_logging

# Flask App Setup
app = Flask(__name__)
app.secret_key = secrets.token_hex(32)
bcrypt = Bcrypt(app)

# Database configuration
DB_NAME = 'icap.db'

# ICAP Server Configuration
ICAP_HOST = '0.0.0.0'  # Default, will be updated from database or config file
ICAP_PORT = 1344

# Configuration priority: config.py > database > defaults
USE_CONFIG_FILE = True  # Set to False to use only database settings

# Global variables
icap_server_running = False
icap_server_socket = None
logger = None

# ==================== ICAP Server Functions ====================

def load_config_from_file():
    """Load configuration from config.py file"""
    try:
        import importlib.util
        spec = importlib.util.spec_from_file_location("config", "config.py")
        config_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(config_module)
        
        config = {
            'server_host': getattr(config_module, 'SERVER_HOST', '0.0.0.0'),
            'server_port': getattr(config_module, 'SERVER_PORT', 1344),
            'max_file_size': getattr(config_module, 'MAX_FILE_SIZE', 52428800),
            'file_timeout': getattr(config_module, 'FILE_TIMEOUT', 15),
            'log_level': getattr(config_module, 'LOG_LEVEL', 'INFO'),
            'log_to_file': getattr(config_module, 'LOG_TO_FILE', True),
            'log_file': getattr(config_module, 'LOG_FILE', 'icap_server.log'),
            'enable_filename_sanitization': getattr(config_module, 'ENABLE_FILENAME_SANITIZATION', True),
            'enable_size_limits': getattr(config_module, 'ENABLE_SIZE_LIMITS', True),
            'enable_conflict_resolution': getattr(config_module, 'ENABLE_CONFLICT_RESOLUTION', True),
            'icap_version': getattr(config_module, 'ICAP_VERSION', 'ICAP/1.0'),
            'server_name': getattr(config_module, 'SERVER_NAME', 'FakeICAP/1.0'),
            'istag': getattr(config_module, 'ISTAG', '"FakeICAP-001"')
        }
        
        print(f"Loaded configuration from config.py: host={config['server_host']}, port={config['server_port']}")
        return config
        
    except FileNotFoundError:
        print("config.py not found, using database settings")
        return None
    except Exception as e:
        print(f"Error loading config.py: {e}")
        return None

def get_server_host():
    """Get server host from config file or database settings"""
    if USE_CONFIG_FILE:
        config = load_config_from_file()
        if config:
            host = config['server_host']
            # If it's the placeholder, use default
            if host == 'YOUR_SERVER_IP':
                return '0.0.0.0'
            return host
    
    # Fallback to database
    try:
        conn = get_db_connection()
        setting = conn.execute('SELECT value FROM settings WHERE key = ?', ('server_host',)).fetchone()
        conn.close()
        
        if setting:
            host = setting['value']
            # If it's the placeholder, use default
            if host == 'YOUR_SERVER_IP':
                return '0.0.0.0'
            return host
        else:
            return '0.0.0.0'
    except Exception as e:
        print(f"Error getting server host from database: {e}")
        return '0.0.0.0'

def get_server_port():
    """Get server port from config file or database settings"""
    if USE_CONFIG_FILE:
        config = load_config_from_file()
        if config:
            return config['server_port']
    
    # Fallback to database
    try:
        conn = get_db_connection()
        setting = conn.execute('SELECT value FROM settings WHERE key = ?', ('server_port',)).fetchone()
        conn.close()
        
        if setting:
            try:
                return int(setting['value'])
            except ValueError:
                return 1344
        else:
            return 1344
    except Exception as e:
        print(f"Error getting server port from database: {e}")
        return 1344

def sanitize_filename(filename):
    """Sanitize filename to prevent security issues"""
    if not filename:
        return "unknown_file"
    
    # Remove path separators and dangerous characters
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    filename = re.sub(r'\.\.', '_', filename)
    
    # Limit length
    if len(filename) > 100:
        name, ext = os.path.splitext(filename)
        filename = name[:95] + ext
    
    return filename.strip()

def create_date_directory_structure(base_path):
    """Create directory structure: saved_files/YYYY/MM/DD/"""
    try:
        now = datetime.now()
        year = str(now.year)
        month = f"{now.month:02d}"
        day = f"{now.day:02d}"
        
        # Create the full directory path
        dir_path = os.path.join(base_path, year, month, day)
        
        # Create directories if they don't exist
        os.makedirs(dir_path, exist_ok=True)
        
        print(f"Created/verified directory structure: {dir_path}")
        return dir_path
        
    except Exception as e:
        print(f"Error creating directory structure: {e}")
        return base_path

def parse_encapsulated_header(data_str):
    """Parse the Encapsulated header according to RFC 3507"""
    try:
        lines = data_str.split('\r\n')
        for line in lines:
            if line.startswith('Encapsulated:'):
                encapsulated = line[12:].strip()
                logger.icap_logger.info(f"Found Encapsulated header: {encapsulated}")
                return encapsulated
        return None
    except Exception as e:
        logger.icap_logger.error(f"Error parsing Encapsulated header: {e}")
        return None

def extract_encapsulated_content(data, encapsulated_header):
    """Extract content based on Encapsulated header according to RFC 3507"""
    try:
        if not encapsulated_header:
            logger.icap_logger.error("No Encapsulated header found")
            return None
        
        # Parse the encapsulated header
        # Format: req-body=0, res-body=1234, null-body=0
        parts = encapsulated_header.split(',')
        offsets = {}
        
        for part in parts:
            part = part.strip()
            if '=' in part:
                key, value = part.split('=', 1)
                offsets[key.strip()] = int(value.strip())
        
        logger.icap_logger.info(f"Parsed encapsulated offsets: {offsets}")
        
        # Find the content we want
        content_offset = None
        content_type = None
        
        if 'req-body' in offsets:
            content_offset = offsets['req-body']
            content_type = 'req-body'
        elif 'res-body' in offsets:
            content_offset = offsets['res-body']
            content_type = 'res-body'
        elif 'null-body' in offsets:
            # No content to process
            logger.icap_logger.info("Null-body request - no content to process")
            return None
        
        if content_offset is None:
            logger.icap_logger.error("No body content found in encapsulated header")
            return None
        
        # Find the end of ICAP headers (double CRLF)
        header_end = data.find(b'\r\n\r\n')
        if header_end == -1:
            logger.icap_logger.error("Could not find end of ICAP headers")
            return None
        
        # Calculate the actual start of the content
        # The offset is relative to the start of the encapsulated message
        # which is right after the ICAP headers
        content_start = header_end + 4 + content_offset
        
        if content_start >= len(data):
            logger.icap_logger.error(f"Content offset {content_offset} is beyond data length")
            return None
        
        # Extract the content
        content = data[content_start:]
        
        logger.icap_logger.info(f"Extracted {len(content)} bytes of {content_type} content")
        logger.icap_logger.info(f"Content starts with (hex): {content[:50].hex()}")
        logger.icap_logger.info(f"Content starts with (text): {content[:50]}")
        
        # SPECIAL HANDLING: Check if content starts with "2000" (GoAnywhere protocol contamination)
        if content.startswith(b'2000'):
            logger.icap_logger.warning("‚ö†Ô∏è Content starts with '2000' - removing GoAnywhere protocol contamination")
            
            # Remove the "2000" prefix
            content = content[4:]
            
            # Also remove any leading CR/LF after the "2000"
            while content.startswith(b'\r\n') or content.startswith(b'\n') or content.startswith(b'\r'):
                content = content[2:] if content.startswith(b'\r\n') else content[1:]
            
            logger.icap_logger.info(f"Removed '2000' prefix, remaining content: {len(content)} bytes")
            logger.icap_logger.info(f"Clean content starts with (hex): {content[:50].hex()}")
            logger.icap_logger.info(f"Clean content starts with (text): {content[:50]}")
        
        return content
        
    except Exception as e:
        logger.icap_logger.error(f"Error extracting encapsulated content: {e}")
        return None

def extract_file_info_from_icap(data):
    """Extract file information from ICAP request headers"""
    try:
        data_str = data.decode('utf-8', errors='ignore')
        logger.icap_logger.info(f"ICAP request received: {data_str[:200]}...")
        
        file_info = {
            'method': '',
            'filename': '',
            'content_length': 0,
            'path': '',
            'host': ''
        }
        
        # Parse ICAP request line
        lines = data_str.split('\r\n')
        if lines:
            parts = lines[0].split()
            if len(parts) >= 2:
                file_info['method'] = parts[0]
                logger.icap_logger.info(f"ICAP method: {parts[0]}")
        
        # Look for file information in the encapsulated HTTP request/response
        in_http_request = False
        in_http_response = False
        
        for line in lines:
            line = line.strip()
            
            # Detect HTTP request
            if line.startswith('GET ') or line.startswith('POST '):
                in_http_request = True
                # Extract filename from GET/POST line
                parts = line.split()
                if len(parts) >= 2:
                    path = parts[1]
                    file_info['path'] = path
                    # Extract filename from path
                    if '/' in path:
                        filename = path.split('/')[-1]
                        file_info['filename'] = filename
                continue
            
            # Detect HTTP response
            elif line.startswith('HTTP/1.1 200 OK') or line.startswith('HTTP/1.1 200 OK'):
                in_http_response = True
                continue
            
            # Parse headers in HTTP request or response
            if in_http_request or in_http_response:
                if ':' in line:
                    header_name, header_value = line.split(':', 1)
                    header_name = header_name.strip().lower()
                    header_value = header_value.strip()
                    
                    if header_name == 'content-length':
                        try:
                            file_info['content_length'] = int(header_value)
                            logger.icap_logger.info(f"Found Content-Length: {header_value}")
                        except ValueError:
                            logger.icap_logger.warning(f"Invalid Content-Length: {header_value}")
                    elif header_name == 'host' and not file_info['host']:
                        file_info['host'] = header_value
                        logger.icap_logger.info(f"Found Host: {header_value}")
                    elif header_name == 'content-disposition':
                        # Extract filename from Content-Disposition header
                        # Example: Content-Disposition: form-data; name="file"; filename="gnome_farmer.png"
                        if 'filename=' in header_value:
                            filename_part = header_value.split('filename=')[1].strip()
                            # Remove quotes if present
                            if filename_part.startswith('"') and filename_part.endswith('"'):
                                filename_part = filename_part[1:-1]
                            file_info['filename'] = filename_part
                            logger.icap_logger.info(f"Found filename in Content-Disposition: {filename_part}")
                    elif header_name == 'content-type' and 'multipart' in header_value:
                        # This might be a multipart upload, look for filename in the boundary data
                        logger.icap_logger.info(f"Found multipart Content-Type: {header_value}")
                elif line == '':  # End of HTTP headers
                    break
        
        logger.icap_logger.info(f"Extracted file info: {file_info}")
        return file_info
    except Exception as e:
        logger.icap_logger.error(f"Error extracting file info: {e}")
        return None

def receive_file_content(client_socket, file_info):
    """Try to receive the actual file content from GoAnywhere"""
    try:
        print("Attempting to receive file content...")
        
        # Set a reasonable timeout for file content
        client_socket.settimeout(15.0)
        
        # Try to receive more data (the actual file content)
        raw_data = b""
        expected_size = file_info.get('content_length', 0)
        
        try:
            while len(raw_data) < expected_size or expected_size == 0:
                chunk = client_socket.recv(8192)
                if not chunk:
                    break
                raw_data += chunk
                print(f"Received chunk: {len(chunk)} bytes (total: {len(raw_data)})")
                
                # Safety limit to prevent memory issues
                if len(raw_data) > 50000000:  # 50MB limit
                    print("Reached safety size limit, stopping reception")
                    break
                
                # If we have the expected size, stop
                if expected_size > 0 and len(raw_data) >= expected_size:
                    print("Received expected amount of data")
                    break
                    
        except socket.timeout:
            print("Timeout receiving file content")
        except Exception as e:
            print(f"Error receiving file content: {e}")
        
        if raw_data:
            print(f"Total raw data received: {len(raw_data)} bytes")
            
            # Filter out ICAP protocol data and extract clean file content
            file_content = filter_icap_protocol_data(raw_data)
            if file_content:
                print(f"Filtered file content: {len(file_content)} bytes")
                return file_content
            else:
                print("Failed to filter ICAP protocol data")
                return None
        else:
            print("No file content received")
            return None
            
    except Exception as e:
        print(f"Error in receive_file_content: {e}")
        return None

def filter_icap_protocol_data(data):
    """Filter out ICAP protocol data and extract clean file content - RFC 3507 compliant"""
    try:
        logger = get_logger()
        logger.icap_logger.info(f"Filtering ICAP data: {len(data)} bytes")
        logger.icap_logger.info(f"First 100 bytes (hex): {data[:100].hex()}")
        
        # Check if data starts with "2000" - this indicates protocol contamination
        data_str = data.decode('utf-8', errors='ignore')
        logger.icap_logger.info(f"First 50 characters (text): '{data_str[:50]}'")
        
        if data_str.startswith('2000'):
            logger.icap_logger.warning("‚ö†Ô∏è Data starts with '2000' - this indicates ICAP protocol contamination")
            logger.icap_logger.info("No real ICAP headers found - extracting pure file content after '2000'")
            
            # NEW APPROACH: Simple ICAP chunked transfer reconstruction
            # Focus on the core issue: ICAP client mixing protocol data with binary chunks
            # This should work for ANY binary file type, not just PNGs
            
            original_size = len(data)
            logger.icap_logger.info(f"Reconstructing binary file from ICAP chunks: {original_size} bytes")
            
            # Step 1: Split by "2000" markers to identify chunks
            chunks = data.split(b'2000')
            logger.icap_logger.info(f"Found {len(chunks)} segments separated by '2000' markers")
            
            # Step 2: Reconstruct the original binary file by extracting clean data from each chunk
            clean_data = bytearray()
            
            for i, chunk in enumerate(chunks):
                if i == 0:
                    # First chunk starts with "2000", so the first segment is empty or has leading data
                    continue
                
                logger.icap_logger.info(f"Processing chunk {i}: {len(chunk)} bytes")
                
                # Remove leading CR/LF from chunk (after "2000")
                chunk_start = 0
                while chunk_start < len(chunk) and chunk[chunk_start:chunk_start+2] == b'\r\n':
                    chunk_start += 2
                if chunk_start < len(chunk) and chunk[chunk_start] in b'\r\n':
                    chunk_start += 1
                
                # Remove trailing protocol data from chunk
                chunk_end = len(chunk)
                
                # Look for HTTP headers at the end of the chunk
                chunk_str = chunk.decode('utf-8', errors='ignore')
                http_header_patterns = [
                    'HTTP/1.1 200 OK',
                    'Host: www.example.com',
                    'User-Agent: GoAnywhere',
                    'Content-Length:',
                    'Content-Type:',
                    'Accept-Ranges:',
                    'Last-Modified:',
                    'ETag:',
                    'Server:',
                    'Connection:',
                    'X-Client-IP:',
                    'X-Server-IP:',
                ]
                
                # Find the first HTTP header pattern
                earliest_header_pos = -1
                for pattern in http_header_patterns:
                    pos = chunk_str.find(pattern)
                    if pos != -1 and (earliest_header_pos == -1 or pos < earliest_header_pos):
                        earliest_header_pos = pos
                
                if earliest_header_pos != -1:
                    # Remove everything from the HTTP header onwards
                    chunk_end = earliest_header_pos
                    logger.icap_logger.info(f"Found HTTP header at position {earliest_header_pos}, truncating chunk")
                else:
                    # If no HTTP headers found, check for trailing CR/LF that might be protocol contamination
                    # Look for patterns like "\r\n" at the end that don't belong to the binary data
                    if chunk_end > 2:
                        # Check if the last 2 bytes are CR/LF
                        if chunk[chunk_end-2:chunk_end] == b'\r\n':
                            # Check if this looks like protocol contamination by looking at context
                            # If the byte before CR/LF is binary data, the CR/LF is likely contamination
                            if chunk_end > 3 and (chunk[chunk_end-3] < 32 or chunk[chunk_end-3] > 126):
                                chunk_end -= 2
                                logger.icap_logger.info(f"Removed trailing CR/LF from chunk {i}")
                        elif chunk[chunk_end-1] in b'\r\n':
                            # Check if the last byte is CR or LF
                            if chunk_end > 2 and (chunk[chunk_end-2] < 32 or chunk[chunk_end-2] > 126):
                                chunk_end -= 1
                                logger.icap_logger.info(f"Removed trailing CR/LF from chunk {i}")
                
                # Extract the clean chunk data
                clean_chunk = chunk[chunk_start:chunk_end]
                clean_data.extend(clean_chunk)
                
                logger.icap_logger.info(f"Added {len(clean_chunk)} clean bytes from chunk {i}")
            
            file_content = bytes(clean_data)
            
            # Step 3: Final cleanup - remove any remaining obvious protocol contamination
            # Remove any remaining "2000" sequences (just in case)
            file_content = file_content.replace(b'2000', b'')
            
            # DISABLED: PNG structure-aware cleanup was making things worse
            # The ICAP corruption is too severe for complex reconstruction
            # The simple approach (image displaying but garbled) was the best we could achieve
            
            # Remove trailing protocol contamination for binary files
            if len(file_content) > 100:
                # Generic approach: Look for common file end markers and remove trailing data
                # This works for most binary file formats
                
                # Check for common binary file end markers
                end_markers = [
                    b'\x00\x00\x00\x00IEND\xae\x42\x60\x82',  # PNG IEND
                    b'\xff\xd9',  # JPEG end
                    b'%%EOF',  # PDF end
                    b'\x00\x00\x00\x00\x49\x45\x4E\x44\xAE\x42\x60\x82',  # Alternative PNG IEND
                ]
                
                for marker in end_markers:
                    marker_pos = file_content.rfind(marker)
                    if marker_pos != -1:
                        # Calculate the end position based on marker type
                        if marker == b'\xff\xd9':  # JPEG
                            end_pos = marker_pos + 2
                        elif marker == b'%%EOF':  # PDF
                            end_pos = marker_pos + 5
                        else:  # PNG
                            end_pos = marker_pos + 12
                        
                        if end_pos < len(file_content):
                            trailing_data = file_content[end_pos:]
                            # Check if trailing data looks like protocol contamination
                            if trailing_data.startswith(b'\r\n') or trailing_data.startswith(b'\n') or trailing_data.startswith(b'0'):
                                logger.icap_logger.info(f"Removing trailing protocol data: {len(trailing_data)} bytes")
                                file_content = file_content[:end_pos]
                        break
            
            total_removed = original_size - len(file_content)
            logger.icap_logger.info(f"ICAP chunked transfer parsing removed {total_removed} total bytes")
            
            logger.icap_logger.info(f"‚úÖ Extracted pure file content: {len(file_content)} bytes")
            logger.icap_logger.info(f"Content starts with (hex): {file_content[:50].hex()}")
            logger.icap_logger.info(f"Content starts with (text): {file_content[:50]}")
            return file_content
        else:
            logger.icap_logger.info("Data does not start with '2000' - proceeding with normal ICAP header parsing")
        
        # Convert to string for ICAP protocol detection
        # data_str = data.decode('utf-8', errors='ignore')  # Already done above
        
        # Step 1: Parse ICAP headers to find Encapsulated header (RFC 3507 Section 4.4.1)
        encapsulated_header = None
        icap_header_end = None
        
        # Find the end of ICAP headers (double CRLF) - try multiple patterns
        header_end = data_str.find('\r\n\r\n')
        if header_end == -1:
            # Try with just LF
            header_end = data_str.find('\n\n')
            if header_end == -1:
                # Try with just CR
                header_end = data_str.find('\r\r')
                if header_end == -1:
                    logger.icap_logger.error("‚ùå No ICAP header boundary found")
                    logger.icap_logger.error(f"First 200 bytes: {data[:200]}")
                    return None
        
        icap_header_end = header_end + 4  # Include the \r\n\r\n
        icap_headers = data_str[:header_end]
        
        logger.icap_logger.info(f"ICAP headers end at position: {icap_header_end}")
        logger.icap_logger.info(f"ICAP headers:\n{icap_headers}")
        logger.icap_logger.info(f"Next 50 bytes after headers: {data[icap_header_end:icap_header_end+50]}")
        
        # Parse Encapsulated header - be more flexible with parsing
        encapsulated_header = None
        for line in icap_headers.split('\r\n'):
            line = line.strip()
            logger.icap_logger.info(f"Parsing ICAP header line: '{line}'")
            if line.lower().startswith('encapsulated:'):
                # Extract everything after 'Encapsulated:' (case-insensitive)
                encapsulated_header = line[len('Encapsulated:'):].strip()
                logger.icap_logger.info(f"üîç Found Encapsulated header: '{encapsulated_header}'")
                break
        
        if not encapsulated_header:
            logger.icap_logger.warning("‚ùå No Encapsulated header found - using fallback extraction")
            # Fallback: Look for file content directly in the data
            return extract_file_content_fallback(data, icap_header_end)
        
        # Step 2: Parse Encapsulated header according to RFC 3507
        logger.icap_logger.info(f"Attempting RFC 3507 compliant extraction with: {encapsulated_header}")
        file_content = extract_encapsulated_content(data, encapsulated_header)
        
        if file_content:
            logger.icap_logger.info(f"‚úÖ RFC 3507 extraction successful: {len(file_content)} bytes")
            return file_content
        else:
            logger.icap_logger.warning("‚ùå RFC 3507 extraction failed - using fallback extraction")
            return extract_file_content_fallback(data, icap_header_end)
        
    except Exception as e:
        logger.icap_logger.error(f"Error filtering ICAP data: {e}")
        return None

def extract_file_content_fallback(data, icap_header_end):
    """Aggressive fallback extraction - find actual file content"""
    try:
        print(f"Using aggressive fallback extraction from position {icap_header_end}")
        
        # Look at the entire remaining data
        remaining_data = data[icap_header_end:]
        
        print(f"Remaining data length: {len(remaining_data)} bytes")
        print(f"First 100 bytes (hex): {remaining_data[:100].hex()}")
        print(f"First 100 bytes (text): {remaining_data[:100]}")
        
        # Strategy 1: Look for file signatures with position context
        signatures = [
            (b'%PDF', 'PDF'),
            (b'\x89PNG\r\n\x1a\n', 'PNG'),
            (b'\xff\xd8\xff', 'JPEG'),
            (b'GIF87a', 'GIF'),
            (b'GIF89a', 'GIF'),
            (b'PK\x03\x04', 'ZIP'),
            (b'RIFF', 'RIFF'),
            (b'ID3', 'MP3'),  # MP3 ID3 tag
            (b'\xff\xfb', 'MP3'),  # MP3 frame sync
            (b'ftyp', 'MP4'),
        ]
        
        # Find all signature positions and analyze context
        signature_candidates = []
        
        for signature, file_type in signatures:
            pos = remaining_data.find(signature)
            if pos != -1:
                # Analyze context around this signature
                context_start = max(0, pos - 50)
                context_end = min(len(remaining_data), pos + len(signature) + 50)
                context = remaining_data[context_start:context_end]
                
                # Calculate how "clean" this position looks
                score = calculate_signature_score(remaining_data, pos, signature, file_type)
                
                signature_candidates.append({
                    'signature': signature,
                    'file_type': file_type,
                    'position': pos,
                    'score': score,
                    'context': context
                })
                
                print(f"Found {file_type} signature at position {icap_header_end + pos}, score: {score:.2f}")
        
        # Sort by score (highest first)
        signature_candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # Try the best candidates
        for candidate in signature_candidates:
            if candidate['score'] > 0.3:  # Reasonable threshold
                content = remaining_data[candidate['position']:]
                
                print(f"Trying {candidate['file_type']} content at position {icap_header_end + candidate['position']}")
                print(f"Content length: {len(content)} bytes")
                print(f"Content starts with (hex): {content[:50].hex()}")
                
                # Additional validation for specific types
                if validate_file_content(content, candidate['file_type']):
                    print(f"‚úÖ Validated {candidate['file_type']} content")
                    return content
        
        # Strategy 2: Look for readable text (for text files)
        print("No good file signatures found, looking for readable text content...")
        
        # Scan through the data looking for readable text sequences
        best_text_pos = None
        best_text_score = 0
        
        for i in range(0, min(2000, len(remaining_data)), 10):  # Check every 10 bytes in first 2KB
            chunk = remaining_data[i:i+100]
            if len(chunk) >= 50:
                text_score = calculate_text_score(chunk)
                if text_score > best_text_score:
                    best_text_score = text_score
                    best_text_pos = i
        
        if best_text_score > 0.7:  # High threshold for text
            text_content = remaining_data[best_text_pos:]
            
            print(f"Found readable text at position {icap_header_end + best_text_pos}, score: {best_text_score:.2f}")
            print(f"Text content starts: {text_content[:100]}")
            
            # Trim any leading non-text characters
            while len(text_content) > 0 and text_content[0] < 32 and text_content[0] not in [9, 10, 13]:  # Not tab, newline, carriage return
                text_content = text_content[1:]
            
            if len(text_content) > 20:
                print(f"‚úÖ Extracted text content: {len(text_content)} bytes")
                return text_content
        
        # Strategy 3: Look for patterns that suggest file boundaries
        print("Looking for file boundary patterns...")
        
        # Look for common patterns that might indicate file content start
        patterns = [
            b'2000\r\n',  # ICAP status followed by content
            b'2000\n',
            b'\r\n\r\n',  # Double newline (header boundary)
            b'\n\n',      # Double newline (Unix style)
        ]
        
        for pattern in patterns:
            pos = remaining_data.find(pattern)
            if pos != -1:
                after_pattern = remaining_data[pos + len(pattern):]
                
                # Skip whitespace
                while len(after_pattern) > 0 and after_pattern[0] in [10, 13, 32]:
                    after_pattern = after_pattern[1:]
                
                if len(after_pattern) > 100:
                    # Check if this looks like file content
                    if looks_like_file_content(after_pattern):
                        print(f"Found content after pattern {pattern} at position {icap_header_end + pos}")
                        print(f"Content starts with (hex): {after_pattern[:50].hex()}")
                        return after_pattern
        
        print("‚ùå No suitable file content found in aggressive fallback")
        return None
        
    except Exception as e:
        print(f"Error in aggressive fallback extraction: {e}")
        return None

def calculate_signature_score(data, pos, signature, file_type):
    """Calculate how likely this signature position is the real file start"""
    try:
        score = 0.5  # Base score for finding a signature
        
        # Check what's before the signature
        before = data[:pos]
        after = data[pos + len(signature):pos + len(signature) + 50]
        
        # Penalize if there's too much readable text before (might be headers)
        if len(before) > 100:
            text_ratio = calculate_text_score(before[:100])
            if text_ratio > 0.8:
                score -= 0.3
        
        # Reward if signature is at a reasonable position (not too early, not too late)
        if 50 < pos < 1000:
            score += 0.2
        elif pos > 5000:
            score -= 0.2
        
        # Reward if content after signature looks like file content
        if len(after) > 20:
            if file_type in ['PDF', 'PNG', 'JPEG']:
                # For binary files, reward binary content
                binary_ratio = sum(1 for b in after if b < 32 or b > 126) / len(after)
                score += binary_ratio * 0.3
            else:
                # For other files, check if it looks reasonable
                if looks_like_file_content(data[pos:]):
                    score += 0.2
        
        return max(0.0, min(1.0, score))
        
    except Exception as e:
        print(f"Error calculating signature score: {e}")
        return 0.0

def calculate_text_score(data):
    """Calculate how likely data is readable text"""
    try:
        if len(data) == 0:
            return 0.0
        
        text_chars = 0
        total_chars = len(data)
        
        for byte in data:
            # Printable ASCII characters plus common whitespace
            if (32 <= byte <= 126) or byte in [9, 10, 13]:  # space, tab, newline, carriage return
                text_chars += 1
        
        return text_chars / total_chars
        
    except Exception as e:
        print(f"Error calculating text score: {e}")
        return 0.0

def validate_file_content(content, file_type):
    """Basic validation of extracted file content"""
    try:
        if len(content) < 20:
            return False
        
        if file_type == 'PDF':
            return content.startswith(b'%PDF')
        elif file_type == 'PNG':
            return content.startswith(b'\x89PNG')
        elif file_type == 'JPEG':
            return content.startswith(b'\xff\xd8\xff')
        elif file_type == 'GIF':
            return content.startswith(b'GIF')
        else:
            # For other types, just check if it looks like file content
            return looks_like_file_content(content)
            
    except Exception as e:
        print(f"Error validating file content: {e}")
        return False

def looks_like_file_content(data):
    """Check if data looks like file content vs protocol data"""
    try:
        if len(data) < 50:
            return False
        
        sample = data[:100]
        
        # Check for protocol indicators
        sample_str = sample.decode('utf-8', errors='ignore').lower()
        protocol_indicators = ['http/', 'icap/', 'content-', '2000', '100 ', '204 ', '400 ']
        
        protocol_count = sum(sample_str.count(indicator) for indicator in protocol_indicators)
        
        # If too many protocol indicators, probably not file content
        if protocol_count > 3:
            return False
        
        # Check for reasonable binary/structured content
        binary_ratio = sum(1 for b in sample if b < 32 or b > 126) / len(sample)
        
        # Either mostly binary or mostly printable text is good
        return binary_ratio > 0.1 or binary_ratio < 0.9
        
    except Exception as e:
        print(f"Error checking file content: {e}")
        return False

def find_file_signature_in_content(data):
    """Find actual file signatures within content data"""
    try:
        if len(data) < 10:
            return None
        
        print(f"Searching for file signatures in {len(data)} bytes...")
        print(f"First 100 bytes (hex): {data[:100].hex()}")
        print(f"First 100 bytes (text): {data[:100]}")
        
        # Common file signatures with their positions and priorities
        # Higher priority = more likely to be the actual file content
        signatures = [
            (b'%PDF', 'PDF', 10),           # PDF files - high priority
            (b'\x89PNG\r\n\x1a\n', 'PNG', 9),  # PNG files - high priority
            (b'\xff\xd8\xff', 'JPEG', 8),   # JPEG files - high priority
            (b'GIF87a', 'GIF87a', 7),       # GIF files
            (b'GIF89a', 'GIF89a', 7),       # GIF files
            (b'PK\x03\x04', 'ZIP/Office', 6),  # ZIP/Office files
            (b'PK\x05\x06', 'ZIP', 6),      # ZIP files
            (b'RIFF', 'RIFF (WAV/AVI)', 5), # RIFF files
            (b'\x1f\x8b\x08', 'GZIP', 4),   # GZIP files
            (b'ftypqt', 'MOV', 3),          # MOV files
            (b'\x1a\x45\xdf\xa3', 'WebM', 3), # WebM files
            (b'\x49\x49\x2a\x00', 'TIFF (little endian)', 2), # TIFF
            (b'\x4d\x4d\x00\x2a', 'TIFF (big endian)', 2),   # TIFF
            (b'\x00\x00\x01\x00', 'ICO', 2),  # ICO files
            (b'MZ', 'EXE', 1),              # EXE files - lowest priority (often embedded)
            (b'\x7fELF', 'ELF', 1),         # ELF files - lowest priority (often embedded)
        ]
        
        # Find all signatures and their positions
        found_signatures = []
        
        for signature, file_type, priority in signatures:
            pos = data.find(signature)
            if pos != -1:
                found_signatures.append({
                    'signature': signature,
                    'file_type': file_type,
                    'position': pos,
                    'priority': priority
                })
                print(f"Found {file_type} signature at position: {pos} (priority: {priority})")
        
        if not found_signatures:
            print("No known file signatures found")
            return None
        
        # Sort by priority first, then by position
        found_signatures.sort(key=lambda x: (-x['priority'], x['position']))
        
        # Print the sorted results
        print("Signatures sorted by priority:")
        for sig in found_signatures:
            print(f"  {sig['file_type']} at position {sig['position']} (priority: {sig['priority']})")
        
        # Try the highest priority signatures first
        for sig in found_signatures:
            signature, file_type, pos = sig['signature'], sig['file_type'], sig['position']
            content = data[pos:]
            
            print(f"Trying {file_type} content from position {pos}")
            
            # Additional validation for specific formats
            if file_type == 'PNG':
                result = extract_valid_png_content(data, pos)
                if result:
                    print(f"‚úÖ Valid PNG content extracted: {len(result)} bytes")
                    return result
            elif file_type == 'JPEG':
                result = extract_valid_jpeg_content(data, pos)
                if result:
                    print(f"‚úÖ Valid JPEG content extracted: {len(result)} bytes")
                    return result
            elif file_type == 'PDF':
                result = extract_valid_pdf_content(data, pos)
                if result:
                    print(f"‚úÖ Valid PDF content extracted: {len(result)} bytes")
                    return result
            else:
                # For other formats, do basic validation
                if len(content) > 100:
                    # Check if this looks like reasonable file content
                    score = score_file_content(content)
                    if score > 0.2:  # Higher threshold for non-validated formats
                        result = remove_trailing_icap_data(content)
                        print(f"‚úÖ {file_type} content extracted: {len(result)} bytes (score: {score:.2f})")
                        return result
                    else:
                        print(f"‚ö†Ô∏è  {file_type} content score too low: {score:.2f}")
        
        print("No valid file content found from any signature")
        return None
        
    except Exception as e:
        print(f"Error finding file signature: {e}")
        return None

def remove_trailing_icap_data(data):
    """Remove any ICAP protocol data from the end of file content"""
    try:
        data_str = data.decode('utf-8', errors='ignore')
        
        # Look for ICAP protocol patterns at the end
        icap_end_patterns = [
            '\r\n\r\nICAP/',
            '\n\nICAP/',
            '\r\nICAP/',
            '\nICAP/',
            'ICAP/1.0',
            '2000\r\n',
            '2000\n'
        ]
        
        best_end = len(data)
        
        for pattern in icap_end_patterns:
            pos = data_str.rfind(pattern)  # Search from the end
            if pos != -1 and pos < best_end:
                # Make sure this is actually at the end (not in the middle)
                remaining_after = data_str[pos + len(pattern):]
                if len(remaining_after.strip()) < 100:  # Not much content after pattern
                    best_end = pos
                    print(f"Found trailing ICAP pattern at position: {pos}")
        
        return data[:best_end]
        
    except Exception as e:
        print(f"Error removing trailing ICAP data: {e}")
        return data

def score_file_content(data):
    """Score how likely this data is to be file content vs ICAP protocol"""
    try:
        if len(data) < 50:
            return 0.0
        
        score = 0.0
        data_str = data.decode('utf-8', errors='ignore')
        
        # Factor 1: Binary content (good for files)
        binary_bytes = sum(1 for b in data[:1000] if b < 32 or b > 126)
        binary_ratio = binary_bytes / min(1000, len(data))
        score += binary_ratio * 0.3  # Up to 0.3 points for binary content
        
        # Factor 2: Not too much printable ASCII that looks like protocol
        printable_chars = sum(1 for b in data[:1000] if 32 <= b <= 126)
        printable_ratio = printable_chars / min(1000, len(data))
        
        # Check if printable content looks like HTTP/ICAP protocol
        protocol_indicators = ['HTTP/', 'ICAP/', 'Content-', '2000', '100 ', '204 ', '400 ']
        protocol_count = sum(data_str.count(indicator) for indicator in protocol_indicators)
        protocol_ratio = protocol_count / max(1, len(data_str.split()))
        
        if printable_ratio > 0.8 and protocol_ratio > 0.1:
            score -= 0.4  # Penalize content that looks like protocol
        else:
            score += (1 - protocol_ratio) * 0.2  # Reward non-protocol content
        
        # Factor 3: Structure and patterns
        # Check for repeating patterns (common in files)
        if len(data) > 100:
            sample = data[:100]
            pattern_score = 0
            for i in range(10, len(sample)):
                if sample[i] == sample[i-10]:
                    pattern_score += 1
            pattern_ratio = pattern_score / (len(sample) - 10)
            score += pattern_ratio * 0.1
        
        # Factor 4: Known file signatures (bonus)
        signatures = [
            b'\x89PNG', b'\xff\xd8\xff', b'%PDF', b'GIF8', b'RIFF', 
            b'PK\x03\x04', b'\x1f\x8b\x08', b'MZ', b'\x7fELF'
        ]
        
        for sig in signatures:
            if data.startswith(sig):
                score += 0.5
                break
        
        # Factor 5: Size (larger is more likely to be file content)
        if len(data) > 1000:
            score += 0.1
        elif len(data) > 10000:
            score += 0.2
        
        return max(0.0, min(1.0, score))  # Clamp between 0 and 1
        
    except Exception as e:
        print(f"Error scoring content: {e}")
        return 0.0

def extract_valid_png_content(data, start_pos):
    """Extract valid PNG content by parsing PNG chunk structure"""
    try:
        # PNG structure: signature + IHDR chunk + data chunks + IEND chunk
        png_data = data[start_pos:]
        
        # Minimum PNG size: signature (8 bytes) + IHDR chunk (25 bytes) + IEND chunk (12 bytes) = 45 bytes
        if len(png_data) < 45:
            print("PNG data too small for valid file")
            return None
        
        # Verify PNG signature
        if not png_data.startswith(b'\x89PNG\r\n\x1a\n'):
            print("Invalid PNG signature")
            return None
        
        # Parse PNG chunks to find the end of valid PNG data
        pos = 8  # After signature
        chunk_data = bytearray()
        chunk_data.extend(png_data[:8])  # Add signature
        
        while pos + 8 <= len(png_data):  # Need at least 8 bytes for chunk header
            # Read chunk header: length (4 bytes) + type (4 bytes)
            chunk_length = int.from_bytes(png_data[pos:pos+4], byteorder='big')
            chunk_type = png_data[pos+4:pos+8]
            
            print(f"Found PNG chunk: {chunk_type.decode('ascii', errors='ignore')} ({chunk_length} bytes)")
            
            # Add chunk header
            chunk_data.extend(png_data[pos:pos+8])
            pos += 8
            
            # Check if we have enough data for this chunk
            if pos + chunk_length + 4 > len(png_data):
                print(f"Chunk {chunk_type} extends beyond available data")
                break
            
            # Add chunk data
            chunk_data.extend(png_data[pos:pos+chunk_length])
            pos += chunk_length
            
            # Add CRC (4 bytes)
            if pos + 4 <= len(png_data):
                chunk_data.extend(png_data[pos:pos+4])
                pos += 4
            
            # Check for IEND chunk (marks end of PNG)
            if chunk_type == b'IEND':
                print("Found IEND chunk - PNG complete")
                break
        
        # Verify we got a reasonable PNG
        if len(chunk_data) < 45:
            print("Extracted PNG data too small")
            return None
        
        # Verify it still starts with PNG signature
        if not chunk_data.startswith(b'\x89PNG\r\n\x1a\n'):
            print("Extracted data corrupted - lost PNG signature")
            return None
        
        print(f"Successfully extracted PNG with {len(chunk_data)} bytes")
        return bytes(chunk_data)
        
    except Exception as e:
        print(f"Error extracting PNG content: {e}")
        return None

def extract_valid_jpeg_content(data, start_pos):
    """Extract valid JPEG content by finding JPEG end marker"""
    try:
        jpeg_data = data[start_pos:]
        
        # Verify JPEG signature
        if not jpeg_data.startswith(b'\xff\xd8\xff'):
            print("Invalid JPEG signature")
            return None
        
        # Look for JPEG end marker: \xff\xd9
        end_marker_pos = jpeg_data.find(b'\xff\xd9', 3)  # Start after signature
        if end_marker_pos == -1:
            print("No JPEG end marker found")
            return None
        
        # Extract content including end marker
        jpeg_content = jpeg_data[:end_marker_pos + 2]
        print(f"Extracted JPEG content: {len(jpeg_content)} bytes")
        return jpeg_content
        
    except Exception as e:
        print(f"Error extracting JPEG content: {e}")
        return None

def extract_valid_pdf_content(data, start_pos):
    """Extract valid PDF content by looking for EOF marker"""
    try:
        pdf_data = data[start_pos:]
        
        # Verify PDF signature
        if not pdf_data.startswith(b'%PDF'):
            print("Invalid PDF signature")
            return None
        
        # Look for PDF EOF marker: %%EOF
        eof_marker = b'%%EOF'
        eof_pos = pdf_data.find(eof_marker)
        if eof_pos == -1:
            # Try alternative EOF markers
            for marker in [b'%EOF', b'%%EOF\r', b'%%EOF\n', b'%%EOF\r\n']:
                eof_pos = pdf_data.find(marker)
                if eof_pos != -1:
                    eof_pos += len(marker)
                    break
            else:
                print("No PDF EOF marker found")
                # Return what we have - might be incomplete PDF
                return pdf_data[:1000000]  # Limit to 1MB for safety
        
        # Extract content including EOF marker
        pdf_content = pdf_data[:eof_pos]
        print(f"Extracted PDF content: {len(pdf_content)} bytes")
        return pdf_content
        
    except Exception as e:
        print(f"Error extracting PDF content: {e}")
        return None

def extract_content_between_markers(data, start_pos, signature):
    """Extract file content between start signature and next ICAP marker"""
    try:
        # Start from the signature position
        content = data[start_pos:]
        
        # Look for ICAP protocol markers that might appear in the content
        # Common ICAP markers: "ICAP/", "2000", "100 Continue", etc.
        icap_markers = [
            b'ICAP/',
            b'2000\r\n',
            b'2000\n',
            b'100 Continue',
            b'204 No Content',
            b'400 Bad Request',
            b'\r\n\r\nICAP/',
            b'\n\nICAP/'
        ]
        
        # Find the first occurrence of any ICAP marker after the signature
        earliest_marker_pos = len(content)  # Default to end of content
        
        for marker in icap_markers:
            marker_pos = content.find(marker)
            if marker_pos != -1 and marker_pos > 0:  # Not at position 0 (that would be our signature)
                # Make sure we're not cutting off too early - check if this looks like a real ICAP marker
                if marker_pos < earliest_marker_pos:
                    # Additional validation: check if there's reasonable context around the marker
                    context_start = max(0, marker_pos - 10)
                    context_end = min(len(content), marker_pos + len(marker) + 10)
                    context = content[context_start:context_end]
                    
                    # If this looks like an ICAP marker (has surrounding protocol data), use it
                    if any(icap_term in context.decode('utf-8', errors='ignore').lower() 
                          for icap_term in ['icap/', 'continue', 'content', 'bad request']):
                        earliest_marker_pos = marker_pos
                        print(f"Found ICAP marker '{marker}' at position {start_pos + marker_pos}")
        
        # Extract content up to the earliest ICAP marker
        if earliest_marker_pos < len(content):
            final_content = content[:earliest_marker_pos]
            print(f"Truncated content at ICAP marker, final size: {len(final_content)} bytes")
        else:
            final_content = content
            print(f"No ICAP markers found in content, using full size: {len(final_content)} bytes")
        
        # Verify the content still starts with the expected signature
        if not final_content.startswith(signature):
            print("‚ö†Ô∏è  Warning: Content doesn't start with expected signature after filtering")
            return None
        
        # Additional validation: check for reasonable file structure
        if len(final_content) < 100:  # Files should be at least 100 bytes
            print("‚ö†Ô∏è  Warning: Extracted content seems too small")
            return None
        
        return final_content
        
    except Exception as e:
        print(f"Error extracting content between markers: {e}")
        return None

def save_file_and_record(file_info, file_content, client_address=None, user_agent=None):
    """Save both the file and its record using the same GUID"""
    try:
        if not file_info or not file_content:
            print("Missing file info or content")
            return None, None
        
        # Create base saved_files directory if it doesn't exist
        if not os.path.exists('saved_files'):
            os.makedirs('saved_files')
        
        # Create date-based directory structure
        target_dir = create_date_directory_structure('saved_files')
        
        # Generate GUID for unique identification (used for both file and record)
        shared_guid = str(uuid.uuid4())
        
        # Get original filename and sanitize it
        original_filename = file_info.get('filename', 'unknown_file')
        print(f"Original filename from ICAP: '{original_filename}'")
        safe_filename = sanitize_filename(original_filename)
        print(f"Sanitized filename: '{safe_filename}'")
        
        # Extract file extension from original filename (before sanitization)
        # This ensures we preserve the original extension
        if '.' in original_filename:
            base_name, extension = os.path.splitext(original_filename)
            print(f"Original base name: '{base_name}', extension: '{extension}'")
            # Sanitize only the base name, keep the original extension
            safe_base_name = sanitize_filename(base_name)
            safe_filename = safe_base_name + extension
            print(f"Safe filename with original extension: '{safe_filename}'")
        else:
            # No extension in original filename, use .bin as default
            safe_filename = sanitize_filename(original_filename)
            if not '.' in safe_filename:
                # Global dictionary to track ongoing file transfers
                ongoing_transfers = {}

                def get_transfer_key(file_info, client_address):
                    """Generate a unique key for a file transfer"""
                    return f"{client_address[0]}:{file_info.get('filename', 'unknown')}"

                def save_file_and_record(file_info, file_content, client_address=None, user_agent=None, is_chunk=False, chunk_number=1, total_chunks=1):
                    """Save both the file and its record using the same GUID, with chunk support"""
                    try:
                        if not file_info or not file_content:
                            print("Missing file info or content")
                            return None, None

                        # Get filename from file info
                        original_filename = file_info.get('filename', 'unknown_file')
                        if not original_filename:
                            original_filename = "unknown_file"

                        print(f"Original filename from ICAP: '{original_filename}'")

                        # Handle chunked transfers
                        if is_chunk:
                            transfer_key = get_transfer_key(file_info, client_address)

                            if transfer_key not in ongoing_transfers:
                                # First chunk - initialize transfer
                                ongoing_transfers[transfer_key] = {
                                    'file_info': file_info,
                                    'content': bytearray(),
                                    'chunks_received': 0,
                                    'total_chunks': total_chunks,
                                    'start_time': datetime.now()
                                }
                                print(f"Started new chunked transfer: {transfer_key}")

                            # Append this chunk's content
                            ongoing_transfers[transfer_key]['content'].extend(file_content)
                            ongoing_transfers[transfer_key]['chunks_received'] += 1

                            print(f"Received chunk {chunk_number}/{total_chunks} ({len(file_content)} bytes) for {transfer_key}")
                            print(f"Total content so far: {len(ongoing_transfers[transfer_key]['content'])} bytes")

                            # Check if this is the final chunk
                            if ongoing_transfers[transfer_key]['chunks_received'] >= total_chunks:
                                # Transfer complete - use the accumulated content
                                file_content = bytes(ongoing_transfers[transfer_key]['content'])
                                print(f"Chunked transfer complete: {len(file_content)} bytes total")
                                
                                # Clean up the transfer tracking
                                del ongoing_transfers[transfer_key]
                            else:
                                # More chunks expected - don't save yet
                                print(f"Waiting for more chunks ({ongoing_transfers[transfer_key]['chunks_received']}/{total_chunks})")
                                return None, None
                        
                        # Sanitize filename
                        sanitized_filename = sanitize_filename(original_filename)
                        print(f"Sanitized filename: '{sanitized_filename}'")
                        
                        # Create date-based directory structure
                        date_str = datetime.now().strftime('%Y/%m/%d')
                        dir_path = create_directory_structure(date_str)
                        
                        # Generate GUID for this file
                        shared_guid = str(uuid.uuid4())
                        print(f"Generated GUID: {shared_guid}")
                        
                        # Create filenames
                        safe_filename = f"{shared_guid}_{sanitized_filename}"
        file_filename = safe_filename
        record_filename = f"record_{shared_guid}.txt"

        print(f"Final saved filename will be: {safe_filename}")

        # Full file paths
        file_path = os.path.join(dir_path, file_filename)
        record_path = os.path.join(dir_path, record_filename)

        # Calculate file hash
        file_hash = hashlib.sha256(file_content).hexdigest()
        print(f"File hash: {file_hash}")

        # Save the actual file
        with open(file_path, 'wb') as f:
            f.write(file_content)
        print(f"File saved successfully: {file_path} ({len(file_content)} bytes)")

        # Create and save record file
        record_content = f"""File Record
==========
GUID: {shared_guid}
Original Filename: {original_filename}
Saved Filename: {file_filename}
File Size: {len(file_content)} bytes
File Hash (SHA256): {file_hash}
Client IP: {client_address[0] if client_address else 'Unknown'}
User Agent: {user_agent or 'Unknown'}
Processed At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Date Processed: {datetime.now().strftime('%Y-%m-%d')}
"""

        with open(record_path, 'w', encoding='utf-8') as f:
            f.write(record_content)
        print(f"Record saved successfully: {record_path}")

        file_filename = f"{shared_guid}_{safe_filename}"
        record_filename = f"record_{shared_guid}.txt"
        
        file_path = os.path.join(target_dir, file_filename)
        record_path = os.path.join(target_dir, record_filename)
        
        # Calculate file hash
        file_hash = hashlib.sha256(file_content).hexdigest()
        
        # Save the file content
        try:
            with open(file_path, 'wb') as f:
                f.write(file_content)
            print(f"File saved successfully: {file_filename} ({len(file_content)} bytes")
            print(f"File path: {file_path}")
        except IOError as e:
            print(f"Failed to write file {file_path}: {e}")
            return file_path, None
        
        # Save the record with the same GUID
        record_content = f"""ICAP File Processing Record
Record ID: {shared_guid}
Corresponding File: {file_filename}
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Method: {file_info.get('method', 'Unknown')}
Filename: {file_info.get('filename', 'Unknown')}
Path: {file_info.get('path', 'Unknown')}
Host: {file_info.get('host', 'Unknown')}
Content Length: {file_info.get('content_length', 0)} bytes
File Size: {len(file_content)} bytes
File Hash: {file_hash}
Client IP: {client_address[0] if client_address else 'Unknown'}
User Agent: {user_agent or 'Unknown'}
Status: Processed (204 No Content)
Directory: {target_dir}
Full File Path: {file_path}
"""
        
        try:
            with open(record_path, 'w') as f:
                f.write(record_content)
            print(f"File record saved: {record_filename}")
            print(f"Record path: {record_path}")
        except IOError as e:
            print(f"Failed to write record {record_path}: {e}")
            return file_path, None
        
        # Insert into database
        try:
            conn = get_db_connection()
            cursor = conn.execute('''
                INSERT INTO processed_files 
                (id, original_filename, saved_filename, file_size, file_hash, client_ip, user_agent, processed_at, date_processed)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                shared_guid,  # Use GUID as primary key
                file_info.get('filename', 'Unknown'),
                file_filename,
                len(file_content),
                file_hash,
                client_address[0] if client_address else None,
                user_agent,
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                datetime.now().strftime('%Y-%m-%d')
            ))
            conn.commit()
            conn.close()
            print(f"Database record inserted for {file_info.get('filename', 'Unknown')}")
        except Exception as e:
            print(f"Failed to insert database record: {e}")
        
        return file_path, record_path
        
    except Exception as e:
        print(f"Error in save_file_and_record: {e}")
        return None, None

def get_current_gmt_time():
    """Get current GMT time in ICAP format"""
    return datetime.utcnow().strftime("%a, %d %b %Y %H:%M:%S GMT")

def handle_icap_client(client_socket, client_address):
    """Handle ICAP client connection using proper RFC 3507 protocol"""
    global logger
    
    try:
        logger.icap_logger.info(f"=== ICAP CONNECTION from {client_address} ===")
        
        # Set a reasonable timeout for receiving ICAP request
        client_socket.settimeout(10.0)
        
        # Receive the complete ICAP request
        data = b""
        while True:
            try:
                chunk = client_socket.recv(4096)
                if not chunk:
                    break
                data += chunk
                
                # Look for end of ICAP headers (double CRLF)
                if b'\r\n\r\n' in data:
                    # For RESPMOD with content, we might need to receive more data
                    # But let's start with what we have
                    break
                    
            except socket.timeout:
                logger.icap_logger.warning("Timeout receiving ICAP request")
                break
        
        if not data:
            logger.icap_logger.warning("No data received from ICAP client")
            return
        
        logger.icap_logger.info(f"Received {len(data)} bytes from ICAP client")
        logger.icap_logger.info(f"ICAP request preview: {data[:200]}...")
        
        # Parse the ICAP request
        data_str = data.decode('utf-8', errors='ignore')
        
        if 'OPTIONS' in data_str:
            # Respond to OPTIONS request
            response = f"""ICAP/1.0 200 OK\r
Date: {get_current_gmt_time()}\r
Server: FakeICAP/1.0\r
Connection: close\r
ISTag: "FakeICAP-001"\r
Methods: REQMOD, RESPMOD, OPTIONS\r
Allow: 204\r
Service: FakeICAP/1.0 "Fake ICAP Server"\r
Encapsulated: null-body=0\r
\r
\r
""".encode('utf-8')
            client_socket.send(response)
            logger.icap_logger.info("Sent OPTIONS response")
            return
            
        elif 'RESPMOD' in data_str:
            # Handle RESPMOD request
            logger.icap_logger.info("Processing RESPMOD request")
            
            # Parse the Encapsulated header
            encapsulated_header = parse_encapsulated_header(data_str)
            
            if encapsulated_header:
                # Extract the encapsulated content
                file_content = extract_encapsulated_content(data, encapsulated_header)
                
                if file_content:
                    # Extract file information from the encapsulated HTTP headers
                    file_info = extract_file_info_from_icap(data)
                    
                    if file_info and file_info.get('filename'):
                        logger.icap_logger.info(f"Processing file: {file_info['filename']} ({len(file_content)} bytes)")
                        
                        # Save the file
                        file_path, record_path = save_file_and_record(file_info, file_content, client_address)
                        if file_path and record_path:
                            logger.icap_logger.info(f"Successfully saved file: {file_info['filename']}")
                        else:
                            logger.icap_logger.error(f"Failed to save file: {file_info['filename']}")
                    else:
                        logger.icap_logger.warning("Could not extract file information")
                else:
                    logger.icap_logger.warning("Could not extract encapsulated content")
            else:
                logger.icap_logger.warning("No Encapsulated header found in RESPMOD request")
            
            # Send ICAP response
            response = f"""ICAP/1.0 204 No Content\r
Date: {get_current_gmt_time()}\r
Server: FakeICAP/1.0\r
Connection: close\r
ISTag: "FakeICAP-001"\r
\r
\r
""".encode('utf-8')
            client_socket.send(response)
            logger.icap_logger.info("Sent RESPMOD response")
            
        elif 'REQMOD' in data_str:
            # Handle REQMOD request
            logger.icap_logger.info("Processing REQMOD request")
            
            # Parse the Encapsulated header
            encapsulated_header = parse_encapsulated_header(data_str)
            
            if encapsulated_header:
                # Extract the encapsulated content
                file_content = extract_encapsulated_content(data, encapsulated_header)
                
                if file_content:
                    # Extract file information from the encapsulated HTTP headers
                    file_info = extract_file_info_from_icap(data)
                    
                    if file_info and file_info.get('filename'):
                        logger.icap_logger.info(f"Processing file: {file_info['filename']} ({len(file_content)} bytes)")
                        
                        # Save the file
                        file_path, record_path = save_file_and_record(file_info, file_content, client_address)
                        if file_path and record_path:
                            logger.icap_logger.info(f"Successfully saved file: {file_info['filename']}")
                        else:
                            logger.icap_logger.error(f"Failed to save file: {file_info['filename']}")
                    else:
                        logger.icap_logger.warning("Could not extract file information")
                else:
                    logger.icap_logger.warning("Could not extract encapsulated content")
            else:
                logger.icap_logger.warning("No Encapsulated header found in REQMOD request")
            
            # Send ICAP response
            response = f"""ICAP/1.0 204 No Content\r
Date: {get_current_gmt_time()}\r
Server: FakeICAP/1.0\r
Connection: close\r
ISTag: "FakeICAP-001"\r
\r
\r
""".encode('utf-8')
            client_socket.send(response)
            logger.icap_logger.info("Sent REQMOD response")
            
        else:
            # Unknown ICAP request
            logger.icap_logger.warning(f"Unknown ICAP request: {data_str[:100]}...")
            response = f"""ICAP/1.0 400 Bad Request\r
Date: {get_current_gmt_time()}\r
Server: FakeICAP/1.0\r
Connection: close\r
\r
\r
""".encode('utf-8')
            client_socket.send(response)
            logger.icap_logger.info("Sent 400 Bad Request response")
        
    except Exception as e:
        logger.icap_logger.error(f"Error handling ICAP client: {e}")
        try:
            # Send error response
            response = f"""ICAP/1.0 500 Internal Server Error\r
Date: {get_current_gmt_time()}\r
Server: FakeICAP/1.0\r
Connection: close\r
\r
\r
""".encode('utf-8')
            client_socket.send(response)
        except:
            pass
    finally:
        try:
            client_socket.close()
            logger.icap_logger.info("ICAP connection closed")
        except:
            pass

def run_icap_server():
    """Run the ICAP server in a separate thread"""
    global icap_server_running, icap_server_socket, logger
    
    try:
        # Get server configuration from database
        server_host = get_server_host()
        server_port = get_server_port()
        
        icap_server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        icap_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        icap_server_socket.bind((server_host, server_port))
        icap_server_socket.listen(5)
        
        icap_server_running = True
        print(f"=== ICAP SERVER listening on {server_host}:{server_port} ===")
        if logger:
            logger.log_app(f"ICAP Server started on {server_host}:{server_port}")
        
        while icap_server_running:
            try:
                client_socket, client_address = icap_server_socket.accept()
                # Handle each client in a separate thread
                client_thread = threading.Thread(target=handle_icap_client, args=(client_socket, client_address))
                client_thread.daemon = True
                client_thread.start()
            except Exception as e:
                if icap_server_running:
                    print(f"Error accepting ICAP connection: {e}")
                    if logger:
                        logger.log_error(f"Error accepting ICAP connection: {e}")
                
    except Exception as e:
        print(f"Failed to start ICAP server: {e}")
        if logger:
            logger.log_error(f"Failed to start ICAP server: {e}")
    finally:
        if icap_server_socket:
            icap_server_socket.close()

# ==================== Web Interface Functions ====================

def init_db():
    """Initialize the SQLite database with migration support"""
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    
    # Always create users table if not exists
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE NOT NULL,
            password TEXT NOT NULL,
            is_admin BOOLEAN DEFAULT 0,
            must_change_password BOOLEAN DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_login TIMESTAMP,
            is_default_admin BOOLEAN DEFAULT 0
        )
    ''')
    
    # Always create settings table if not exists
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS settings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            key TEXT UNIQUE NOT NULL,
            value TEXT NOT NULL,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Check if processed_files table exists and has old schema
    cursor.execute("SELECT sql FROM sqlite_master WHERE type='table' AND name='processed_files'")
    table_info = cursor.fetchone()
    
    if table_info and 'id INTEGER PRIMARY KEY' in table_info[0]:
        print("Detected old processed_files table schema, migrating...")
        # Backup existing data
        cursor.execute('SELECT * FROM processed_files')
        existing_data = cursor.fetchall()
        
        # Drop old table
        cursor.execute('DROP TABLE processed_files')
        print("Dropped old processed_files table")
        
        # Create new table with GUID primary key
        cursor.execute('''
            CREATE TABLE processed_files (
                id TEXT PRIMARY KEY,
                original_filename TEXT NOT NULL,
                saved_filename TEXT NOT NULL,
                file_size INTEGER NOT NULL,
                file_hash TEXT,
                client_ip TEXT,
                user_agent TEXT,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                date_processed DATE DEFAULT CURRENT_DATE
            )
        ''')
        
        # Create indexes
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_files_date ON processed_files(date_processed)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_files_filename ON processed_files(original_filename)')
        
        # Migrate existing data (generate GUIDs for old records)
        if existing_data:
            for row in existing_data:
                new_guid = str(uuid.uuid4())
                cursor.execute('''
                    INSERT INTO processed_files 
                    (id, original_filename, saved_filename, file_size, file_hash, client_ip, user_agent, processed_at, date_processed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (new_guid, row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8]))
            print(f"Migrated {len(existing_data)} existing records to new schema")
    else:
        # Create file tracking table with GUID primary key if not exists
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processed_files (
                id TEXT PRIMARY KEY,
                original_filename TEXT NOT NULL,
                saved_filename TEXT NOT NULL,
                file_size INTEGER NOT NULL,
                file_hash TEXT,
                client_ip TEXT,
                user_agent TEXT,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                date_processed DATE DEFAULT CURRENT_DATE
            )
        ''')
        
        # Create index for faster queries
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_files_date ON processed_files(date_processed)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_files_filename ON processed_files(original_filename)')
    
    # Check if default admin exists
    cursor.execute("SELECT * FROM users WHERE username = 'admin'")
    if not cursor.fetchone():
        # Create default admin account
        hashed_password = bcrypt.generate_password_hash('admin').decode('utf-8')
        cursor.execute('''
            INSERT INTO users (username, password, is_admin, must_change_password, is_default_admin)
            VALUES (?, ?, ?, ?, ?)
        ''', ('admin', hashed_password, 1, 1, 1))
    
    # Insert default settings
    default_settings = {
        'server_host': ICAP_HOST,
        'server_port': str(ICAP_PORT),
        'max_file_size': '52428800',  # 50MB
        'file_timeout': '15',
        'theme': 'light',
        'accent_color': 'orange',
        'log_max_bytes': '5242880',   # 5MB
        'log_backup_count': '10',
        'file_history_days': '30'     # Keep 30 days of file history
    }
    
    for key, value in default_settings.items():
        cursor.execute('''
            INSERT OR IGNORE INTO settings (key, value) VALUES (?, ?)
        ''', (key, value))
    
    conn.commit()
    conn.close()
    print("Database initialized successfully")

def get_db_connection():
    """Get database connection"""
    conn = sqlite3.connect(DB_NAME)
    conn.row_factory = sqlite3.Row
    return conn

def login_required(f):
    """Login required decorator"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'user_id' not in session:
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def admin_required(f):
    """Admin required decorator"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'user_id' not in session:
            return redirect(url_for('login'))
        
        conn = get_db_connection()
        user = conn.execute('SELECT is_admin FROM users WHERE id = ?', (session['user_id'],)).fetchone()
        conn.close()
        
        if not user or not user['is_admin']:
            flash('Admin access required', 'error')
            return redirect(url_for('dashboard'))
        return f(*args, **kwargs)
    return decorated_function

# ==================== Flask Routes ====================

@app.route('/')
def index():
    """Redirect to dashboard or login"""
    if 'user_id' in session:
        return redirect(url_for('dashboard'))
    return redirect(url_for('login'))

@app.route('/login', methods=['GET', 'POST'])
def login():
    """Login page"""
    # Get settings for accent color
    conn = get_db_connection()
    settings_data = conn.execute('SELECT key, value FROM settings').fetchall()
    settings_dict = {row['key']: row['value'] for row in settings_data}
    conn.close()
    
    if request.method == 'POST':
        username = request.form.get('username', '').strip()
        password = request.form.get('password', '')
        
        if not username or not password:
            flash('Username and password are required', 'error')
            return render_template('login.html', settings=settings_dict)
        
        conn = get_db_connection()
        user = conn.execute('SELECT * FROM users WHERE username = ?', (username,)).fetchone()
        conn.close()
        
        if user and bcrypt.check_password_hash(user['password'], password):
            session['user_id'] = user['id']
            session['username'] = user['username']
            session['is_admin'] = user['is_admin']
            session['must_change_password'] = user['must_change_password']
            
            # Update last login
            conn = get_db_connection()
            conn.execute('UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?', (user['id'],))
            conn.commit()
            conn.close()
            
            # Log successful login
            if logger:
                logger.log_security(f"User '{username}' logged in successfully from {request.remote_addr}")
            
            if user['must_change_password']:
                flash('You must change your password before continuing', 'warning')
                return redirect(url_for('change_password'))
            
            flash(f'Welcome back, {username}!', 'success')
            return redirect(url_for('dashboard'))
        else:
            # Log failed login attempt
            if logger:
                logger.log_security(f"Failed login attempt for user '{username}' from {request.remote_addr}")
            flash('Invalid username or password', 'error')
    
    return render_template('login.html', settings=settings_dict)

@app.route('/logout')
def logout():
    """Logout"""
    session.clear()
    flash('You have been logged out', 'info')
    return redirect(url_for('login'))

@app.route('/dashboard')
@login_required
def dashboard():
    """Main dashboard"""
    if session.get('must_change_password'):
        return redirect(url_for('change_password'))
    conn = get_db_connection()
    settings_data = conn.execute('SELECT key, value FROM settings').fetchall()
    settings_dict = {row['key']: row['value'] for row in settings_data}
    
    # Get today's file count
    today_count = conn.execute('''
        SELECT COUNT(*) as count FROM processed_files WHERE date_processed = date('now')
    ''').fetchone()
    
    # Get last file processed
    last_file = conn.execute('''
        SELECT original_filename FROM processed_files 
        ORDER BY processed_at DESC LIMIT 1
    ''').fetchone()
    
    # Get server status
    server_status = {
        'uptime': 'Running',
        'files_processed': today_count['count'] if today_count else 0,
        'last_file': last_file['original_filename'] if last_file else 'None'
    }
    
    conn.close()
    
    return render_template('dashboard.html', 
                         settings=settings_dict, 
                         server_status=server_status,
                         theme=settings_dict.get('theme', 'light'),
                         current_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

@app.route('/change_password', methods=['GET', 'POST'])
@login_required
def change_password():
    """Change password page"""
    # Get settings for accent color
    conn = get_db_connection()
    settings_data = conn.execute('SELECT key, value FROM settings').fetchall()
    settings_dict = {row['key']: row['value'] for row in settings_data}
    conn.close()
    
    if request.method == 'POST':
        current_password = request.form.get('current_password', '')
        new_password = request.form.get('new_password', '')
        confirm_password = request.form.get('confirm_password', '')
        
        if not current_password or not new_password:
            flash('All fields are required', 'error')
            return render_template('change_password.html', settings=settings_dict)
        
        if new_password != confirm_password:
            flash('New passwords do not match', 'error')
            return render_template('change_password.html', settings=settings_dict)
        
        if len(new_password) < 6:
            flash('Password must be at least 6 characters long', 'error')
            return render_template('change_password.html', settings=settings_dict)
        
        conn = get_db_connection()
        user = conn.execute('SELECT password FROM users WHERE id = ?', (session['user_id'],)).fetchone()
        
        if not bcrypt.check_password_hash(user['password'], current_password):
            conn.close()
            flash('Current password is incorrect', 'error')
            return render_template('change_password.html', settings=settings_dict)
        
        # Update password
        hashed_password = bcrypt.generate_password_hash(new_password).decode('utf-8')
        conn.execute('''
            UPDATE users 
            SET password = ?, must_change_password = 0 
            WHERE id = ?
        ''', (hashed_password, session['user_id']))
        
        conn.commit()
        conn.close()
        
        session['must_change_password'] = 0
        flash('Password changed successfully!', 'success')
        return redirect(url_for('dashboard'))
    
    return render_template('change_password.html', settings=settings_dict)

def restart_icap_server():
    """Restart the ICAP server with new settings"""
    global icap_server_running, icap_server_socket, logger
    
    try:
        print("Restarting ICAP server with new settings...")
        if logger:
            logger.log_app("Restarting ICAP server with new settings")
        
        # Stop current server
        icap_server_running = False
        if icap_server_socket:
            icap_server_socket.close()
            time.sleep(1)  # Give it time to close
        
        # Start new server
        new_server_thread = threading.Thread(target=run_icap_server)
        new_server_thread.daemon = True
        new_server_thread.start()
        
        if logger:
            logger.log_app(f"ICAP server restarted on {get_server_host()}:{get_server_port()}")
        
        return True
    except Exception as e:
        print(f"Error restarting ICAP server: {e}")
        if logger:
            logger.log_error(f"Error restarting ICAP server: {e}")
        return False

@app.route('/update_settings', methods=['POST'])
@login_required
def update_settings():
    """Update application settings"""
    settings = {
        'server_host': request.form.get('server_host', '0.0.0.0'),
        'server_port': request.form.get('server_port', '1344'),
        'max_file_size': request.form.get('max_file_size', '52428800'),
        'file_timeout': request.form.get('file_timeout', '15'),
        'theme': request.form.get('theme', 'light'),
        'accent_color': request.form.get('accent_color', 'orange'),
        'log_max_bytes': request.form.get('log_max_bytes', '5242880'),
        'log_backup_count': request.form.get('log_backup_count', '10')
    }
    
    conn = get_db_connection()
    for key, value in settings.items():
        conn.execute('''
            INSERT OR REPLACE INTO settings (key, value, updated_at)
            VALUES (?, ?, CURRENT_TIMESTAMP)
        ''', (key, value))
    
    conn.commit()
    conn.close()
    
    # Update logging settings if they changed
    if logger:
        logger.update_settings(
            max_bytes=int(settings['log_max_bytes']),
            backup_count=int(settings['log_backup_count'])
        )
    
    # Check if ICAP server settings changed and restart if needed
    old_host = get_server_host()
    old_port = get_server_port()
    
    if (settings['server_host'] != old_host or 
        int(settings['server_port']) != old_port):
        if restart_icap_server():
            flash('Settings updated and ICAP server restarted!', 'success')
        else:
            flash('Settings updated but ICAP server restart failed', 'warning')
    else:
        flash('Settings updated successfully!', 'success')
    
    return redirect(url_for('dashboard'))

@app.route('/register', methods=['GET', 'POST'])
def register():
    """Registration page"""
    # Get settings for accent color
    conn = get_db_connection()
    settings_data = conn.execute('SELECT key, value FROM settings').fetchall()
    settings_dict = {row['key']: row['value'] for row in settings_data}
    conn.close()
    
    if request.method == 'POST':
        username = request.form.get('username', '').strip()
        password = request.form.get('password', '')
        confirm_password = request.form.get('confirm_password', '')
        
        if not username or not password:
            flash('Username and password are required', 'error')
            return render_template('register.html', settings=settings_dict)
        
        if password != confirm_password:
            flash('Passwords do not match', 'error')
            return render_template('register.html', settings=settings_dict)
        
        if len(password) < 6:
            flash('Password must be at least 6 characters long', 'error')
            return render_template('register.html', settings=settings_dict)
        
        conn = get_db_connection()
        
        # Check if username exists
        existing_user = conn.execute('SELECT id FROM users WHERE username = ?', (username,)).fetchone()
        if existing_user:
            conn.close()
            flash('Username already exists', 'error')
            return render_template('register.html', settings=settings_dict)
        
        # Create new user
        hashed_password = bcrypt.generate_password_hash(password).decode('utf-8')
        conn.execute('''
            INSERT INTO users (username, password, is_admin, must_change_password)
            VALUES (?, ?, ?, ?)
        ''', (username, hashed_password, 0, 0))
        
        conn.commit()
        conn.close()
        
        flash('Account created successfully! Please log in.', 'success')
        return redirect(url_for('login'))
    
    return render_template('register.html', settings=settings_dict)

@app.route('/users')
@admin_required
def users():
    """Users management page"""
    conn = get_db_connection()
    users_list = conn.execute('''
        SELECT id, username, is_admin, must_change_password, created_at, last_login, is_default_admin
        FROM users 
        ORDER BY created_at DESC
    ''').fetchall()
    conn.close()
    
    return render_template('users.html', users=users_list)

@app.route('/add_user', methods=['POST'])
@admin_required
def add_user():
    """Add new user"""
    username = request.form.get('username', '').strip()
    password = request.form.get('password', '')
    is_admin = request.form.get('is_admin') == 'on'
    must_change_password = request.form.get('must_change_password') == 'on'
    
    if not username or not password:
        flash('Username and password are required', 'error')
        return redirect(url_for('users'))
    
    if len(password) < 6:
        flash('Password must be at least 6 characters long', 'error')
        return redirect(url_for('users'))
    
    conn = get_db_connection()
    
    # Check if username exists
    existing_user = conn.execute('SELECT id FROM users WHERE username = ?', (username,)).fetchone()
    if existing_user:
        conn.close()
        flash('Username already exists', 'error')
        return redirect(url_for('users'))
    
    # Create new user
    hashed_password = bcrypt.generate_password_hash(password).decode('utf-8')
    conn.execute('''
        INSERT INTO users (username, password, is_admin, must_change_password)
        VALUES (?, ?, ?, ?)
    ''', (username, hashed_password, is_admin, must_change_password))
    
    conn.commit()
    conn.close()
    
    flash(f'User {username} created successfully!', 'success')
    return redirect(url_for('users'))

@app.route('/delete_user/<int:user_id>')
@admin_required
def delete_user(user_id):
    """Delete user"""
    conn = get_db_connection()
    user = conn.execute('SELECT username, is_default_admin FROM users WHERE id = ?', (user_id,)).fetchone()
    
    if not user:
        conn.close()
        flash('User not found', 'error')
        return redirect(url_for('users'))
    
    if user['is_default_admin']:
        conn.close()
        flash('Cannot delete default admin account', 'error')
        return redirect(url_for('users'))
    
    conn.execute('DELETE FROM users WHERE id = ?', (user_id,))
    conn.commit()
    conn.close()
    
    flash(f'User {user["username"]} deleted successfully!', 'success')
    return redirect(url_for('users'))

@app.route('/reset_user_password/<int:user_id>', methods=['POST'])
@admin_required
def reset_user_password(user_id):
    """Reset user password"""
    new_password = request.form.get('new_password', '')
    must_change_password = request.form.get('must_change_password') == 'on'
    
    if not new_password:
        flash('Password is required', 'error')
        return redirect(url_for('users'))
    
    if len(new_password) < 6:
        flash('Password must be at least 6 characters long', 'error')
        return redirect(url_for('users'))
    
    conn = get_db_connection()
    user = conn.execute('SELECT username FROM users WHERE id = ?', (user_id,)).fetchone()
    
    if not user:
        conn.close()
        flash('User not found', 'error')
        return redirect(url_for('users'))
    
    # Update password
    hashed_password = bcrypt.generate_password_hash(new_password).decode('utf-8')
    conn.execute('''
        UPDATE users 
        SET password = ?, must_change_password = ? 
        WHERE id = ?
    ''', (hashed_password, must_change_password, user_id))
    
    conn.commit()
    conn.close()
    
    flash(f'Password for {user["username"]} reset successfully!', 'success')
    return redirect(url_for('users'))

@app.route('/logs')
@login_required
def logs():
    """Logs page"""
    logger = get_logger()
    log_files = logger.get_log_files()
    
    # Get current settings
    conn = get_db_connection()
    settings_data = conn.execute('SELECT key, value FROM settings').fetchall()
    settings_dict = {row['key']: row['value'] for row in settings_data}
    conn.close()
    
    # Calculate statistics in Python instead of template
    stats = {
        'total_files': len(log_files),
        'total_size_mb': sum(f['size'] for f in log_files) / 1048576,
        'error_logs': len([f for f in log_files if 'error' in f['name']]),
        'security_logs': len([f for f in log_files if 'security' in f['name']]),
        'icap_logs': len([f for f in log_files if 'icap' in f['name']]),
        'web_logs': len([f for f in log_files if 'web' in f['name']]),
        'app_logs': len([f for f in log_files if 'app' in f['name'] and not any(x in f['name'] for x in ['error', 'security', 'icap', 'web'])])
    }
    
    return render_template('logs.html', log_files=log_files, stats=stats, settings=settings_dict)

@app.route('/logs/clear', methods=['POST'])
@admin_required
def clear_logs():
    """Clear all log files"""
    logger = get_logger()
    if logger.clear_logs():
        flash('All log files cleared successfully', 'success')
    else:
        flash('Error clearing log files', 'error')
    
    return redirect(url_for('logs'))

@app.route('/logs/view/<filename>')
@login_required
def view_log(filename):
    """View specific log file content"""
    logger = get_logger()
    log_content = logger.get_log_content(filename, lines=200)
    
    if log_content is None:
        flash('Log file not found', 'error')
        return redirect(url_for('logs'))
    
    # Calculate statistics in Python instead of template
    content_text = ''.join(log_content)
    stats = {
        'total_lines': len(log_content),
        'error_count': content_text.count('ERROR'),
        'warning_count': content_text.count('WARNING'),
        'info_count': content_text.count('INFO')
    }
    
    return render_template('log_view.html', 
                         filename=filename, 
                         log_content=log_content,
                         stats=stats)

@app.route('/logs/download/<filename>')
@login_required
def download_log(filename):
    """Download log file"""
    logger = get_logger()
    log_path = logger.log_dir / filename
    
    if not log_path.exists():
        flash('Log file not found', 'error')
        return redirect(url_for('logs'))
    
    return send_file(log_path, as_attachment=True)

@app.route('/processed_files')
@login_required
def processed_files():
    """Show processed files with date filtering and search"""
    conn = get_db_connection()
    
    # Get file history days setting
    history_days = conn.execute('SELECT value FROM settings WHERE key = ?', ('file_history_days',)).fetchone()
    history_days = int(history_days['value']) if history_days else 30
    
    # Get date parameter (default to today)
    selected_date = request.args.get('date', datetime.now().strftime('%Y-%m-%d'))
    search_term = request.args.get('search', '').strip()
    
    # Build query
    query = '''
        SELECT id, original_filename, saved_filename, file_size, file_hash, 
               client_ip, user_agent, processed_at, date_processed
        FROM processed_files
        WHERE date_processed >= date('now', '-{} days')
    '''.format(history_days)
    
    params = []
    
    # Add date filter if specified
    if selected_date:
        query += ' AND date_processed = ?'
        params.append(selected_date)
    
    # Add search filter if specified
    if search_term:
        query += ' AND original_filename LIKE ?'
        params.append(f'%{search_term}%')
    
    query += ' ORDER BY processed_at DESC'
    
    files = conn.execute(query, params).fetchall()
    
    # Get available dates for dropdown
    dates = conn.execute('''
        SELECT DISTINCT date_processed, COUNT(*) as file_count
        FROM processed_files
        WHERE date_processed >= date('now', '-{} days')
        GROUP BY date_processed
        ORDER BY date_processed DESC
    '''.format(history_days)).fetchall()
    
    # Get today's file count for dashboard
    today_count = conn.execute('''
        SELECT COUNT(*) as count FROM processed_files WHERE date_processed = date('now')
    ''').fetchone()['count']
    
    conn.close()
    
    return render_template('processed_files.html', 
                         files=files, 
                         dates=dates,
                         selected_date=selected_date,
                         search_term=search_term,
                         history_days=history_days,
                         today_count=today_count)

@app.route('/download_processed_file/<file_id>')
@login_required
def download_processed_file(file_id):
    """Download a processed file"""
    conn = get_db_connection()
    file_record = conn.execute('SELECT * FROM processed_files WHERE id = ?', (file_id,)).fetchone()
    conn.close()
    
    if not file_record:
        flash('File not found', 'error')
        return redirect(url_for('processed_files'))
    
    # Search for the file in the date-based directory structure
    file_path = None
    saved_filename = file_record['saved_filename']
    
    # Search in all date directories
    import glob
    search_pattern = os.path.join('saved_files', '**', saved_filename)
    matching_files = glob.glob(search_pattern, recursive=True)
    
    if matching_files:
        file_path = matching_files[0]  # Take the first match
    else:
        flash('File no longer exists on disk', 'error')
        return redirect(url_for('processed_files'))
    
    return send_file(file_path, 
                     download_name=file_record['original_filename'],
                     as_attachment=True)

@app.route('/file_details/<file_id>')
@login_required
def file_details(file_id):
    """Get file details from record file"""
    conn = get_db_connection()
    file_record = conn.execute('SELECT * FROM processed_files WHERE id = ?', (file_id,)).fetchone()
    conn.close()
    
    if not file_record:
        return jsonify({'error': 'File not found'}), 404
    
    # Find the record file
    record_filename = f"record_{file_id}.txt"
    search_pattern = os.path.join('saved_files', '**', record_filename)
    matching_files = glob.glob(search_pattern, recursive=True)
    
    if not matching_files:
        return jsonify({'error': 'Record file not found'}), 404
    
    record_path = matching_files[0]
    
    try:
        with open(record_path, 'r') as f:
            record_content = f.read()
        
        # Parse the record content
        details = {}
        for line in record_content.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                details[key.strip()] = value.strip()
        
        return jsonify(details)
    except Exception as e:
        return jsonify({'error': f'Error reading record: {str(e)}'}), 500

# Add other routes as needed...

# ==================== Main Application ====================

def main():
    """Main application entry point"""
    global logger
    
    print("=" * 60)
    print("FakeICAP Unified Application Starting")
    print("=" * 60)
    
    # Initialize logging
    logger = init_logging()
    logger.log_app("FakeICAP Unified Application starting up")
    
    # Initialize database
    init_db()
    
    # Start ICAP server in background thread
    icap_thread = threading.Thread(target=run_icap_server)
    icap_thread.daemon = True
    icap_thread.start()
    
    # Give ICAP server a moment to start
    time.sleep(1)
    
    print(f"Web interface will be available at: http://localhost:5000")
    print(f"ICAP server running on: {get_server_host()}:{get_server_port()}")
    print("Default credentials: admin / admin")
    print("=" * 60)
    
    # Start Flask web application
    try:
        app.run(debug=False, host='0.0.0.0', port=5000, use_reloader=False)
    except KeyboardInterrupt:
        print("\nShutting down...")
        icap_server_running = False
        if icap_server_socket:
            icap_server_socket.close()
        if logger:
            logger.log_app("FakeICAP Unified Application shutting down")

if __name__ == '__main__':
    main()
